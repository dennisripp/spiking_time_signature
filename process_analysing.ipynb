{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking shapes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 478801.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 538421.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 594094.05it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 612307.15it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 507170.98it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 550433.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 559240.53it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 523633.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 534987.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 576140.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 509017.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 580928.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "Loading and preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.56it/s]\n",
      "\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.43it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title smulti-threaded sample analysis system\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from brian2 import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except ImportError:\n",
    "    !pip3 install ipywidgets\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fixed_timesteps = 1001\n",
    "sub_dirs = ['1_4', '2_4', '3_4', '4_4', '5_4', '7_8']\n",
    "\n",
    "def get_length(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return mfccs.shape[1]\n",
    "\n",
    "def determine_fixed_length(directory):\n",
    "    file_paths = []\n",
    "\n",
    "    for subdir in sub_dirs:\n",
    "        for file in tqdm(os.listdir(os.path.join(directory, subdir))):\n",
    "            file_path = os.path.join(directory, subdir, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    # Utilize multiprocessing for faster computation\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        lengths = list(executor.map(get_length, file_paths))\n",
    "\n",
    "    return min(lengths)\n",
    "\n",
    "def parallel_data_loader(directories):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(parallel_load_and_preprocess, directories), total=len(directories)))\n",
    "    return results\n",
    "\n",
    "def load_and_preprocess_data_subdir(args):\n",
    "    directory, subdir = args\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in os.listdir(os.path.join(directory, subdir)):\n",
    "        file_path = os.path.join(directory, subdir, file)\n",
    "        processed_data = load_audio(file_path)\n",
    "        data.append(processed_data)\n",
    "        label = sub_dirs.index(subdir)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def parallel_load_and_preprocess(directory):\n",
    "    # Create a pool of processes\n",
    "    pool = Pool(cpu_count())\n",
    "\n",
    "    # Create a list of tasks\n",
    "    tasks = [(directory, time_sig) for time_sig in sub_dirs]\n",
    "\n",
    "    # Use imap_unordered to distribute the work among the processes\n",
    "    results = list(tqdm(pool.imap_unordered(load_and_preprocess_data_subdir, tasks), total=len(tasks), mininterval=0.01))\n",
    "\n",
    "    # Close the pool and wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results\n",
    "    combined_data = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    for data, labels in results:\n",
    "        combined_data.extend(data)\n",
    "        combined_labels.extend(labels)\n",
    "    \n",
    "    return combined_data, combined_labels\n",
    "\n",
    "\n",
    "def adjust_fixed_length(features, timesteps):\n",
    "    # If the array is 1-dimensional\n",
    "    if len(features.shape) == 1:\n",
    "        if features.shape[0] > timesteps:\n",
    "            return features[:timesteps]\n",
    "        elif features.shape[0] < timesteps:\n",
    "            padding = np.zeros(timesteps - features.shape[0])\n",
    "            return np.hstack((features, padding))\n",
    "        return features\n",
    "    # If the array is 2-dimensional\n",
    "    else:\n",
    "        # If the time axis of the 2D array is greater than timesteps, crop it.\n",
    "        if features.shape[1] > timesteps:\n",
    "            return features[:, :timesteps]\n",
    "        # If the time axis of the 2D array is less than timesteps, pad it.\n",
    "        elif features.shape[1] < timesteps:\n",
    "            padding = np.zeros((features.shape[0], timesteps - features.shape[1]))\n",
    "            return np.hstack((features, padding))\n",
    "        return features\n",
    "\n",
    "# Convert real-valued features to Poisson spike trains\n",
    "def poisson_spike_encoding(data, duration=10, dt=1*ms):\n",
    "    # Assuming data is normalized between 0 and 1\n",
    "    rates = data * (1.0/dt)\n",
    "    spikes = (np.random.rand(*data.shape) < rates*dt).astype(float)\n",
    "    return spikes\n",
    "\n",
    "def temporal_binning(data, bin_size):\n",
    "    \"\"\"\n",
    "    Bins the data into chunks of bin_size and returns the average of each chunk.\n",
    "    \"\"\"\n",
    "    # Split the data into chunks of bin_size\n",
    "    binned_data = [np.mean(data[i:i+bin_size]) for i in range(0, len(data), bin_size)]\n",
    "    return np.array(binned_data)\n",
    "\n",
    "def rate_based_encoding(data, min_freq, max_freq):\n",
    "    \"\"\"\n",
    "    Convert onset strengths to spike frequencies.\n",
    "    data: The input data (should be normalized to [0, 1])\n",
    "    min_freq: The minimum spike frequency (corresponds to data value of 0)\n",
    "    max_freq: The maximum spike frequency (corresponds to data value of 1)\n",
    "    Returns: Spike frequencies corresponding to input data\n",
    "    \"\"\"\n",
    "    return min_freq + data * (max_freq - min_freq)\n",
    "\n",
    "def extract_bpm_and_instrument(file_path):\n",
    "    match = re.search(r\"instrument_(\\d+)_bpm_(\\d+)_duration_(\\d+)_noise_([\\d.]+)\", file_path)\n",
    "    if match:\n",
    "        instrument = match.group(1)\n",
    "        bpm = match.group(2)\n",
    "        duration = match.group(3)\n",
    "        noise = match.group(4)\n",
    "        return instrument, bpm, duration, noise\n",
    "    return None, None, None, None\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute moving average\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=22050)  # setting sr ensures all files are resampled to this rate\n",
    "    return [y, sr, file_path]\n",
    "\n",
    "# Process the audio file into desired features\n",
    "# Process the audio file into desired features\n",
    "def preprocess_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=22050)  # setting sr ensures all files are resampled to this rate\n",
    "    time_signature = file_path.split('/')[-2].replace('_', '/')\n",
    "    instrument, bpm = extract_bpm_and_instrument(file_path)\n",
    "\n",
    "    # Extracting onset strength\n",
    "    onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_strength, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram_cropped = librosa.feature.tempogram(onset_envelope=onset_strength[20:], sr=sr)\n",
    "    \n",
    "    # Adjust the time axis of each feature to fixed_timesteps\n",
    "    onset_strength_fixed = adjust_fixed_length(onset_strength, fixed_timesteps)\n",
    "    tempogram_fixed = adjust_fixed_length(tempogram, fixed_timesteps)\n",
    "\n",
    "    # Stacking features horizontally\n",
    "    combined_features = np.vstack(poisson_spike_encoding(onset_strength))\n",
    "    \n",
    "    # Normalize to range [0, 1]\n",
    "    encoded_features = (combined_features - np.min(combined_features)) / (np.max(combined_features) - np.min(combined_features))\n",
    "    \n",
    "        # Plotting\n",
    "    plt.figure(figsize=(12, 14))\n",
    "    plt.title('audio  with {time_signature} time signature, {bpm} bpm, and instrument {instrument}')\n",
    "\n",
    "    rows = 6\n",
    "    # 1. Raw audio\n",
    "    plt.subplot(rows, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Raw Audio')\n",
    "\n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 2)\n",
    "    plt.plot(onset_strength_fixed)\n",
    "    plt.title('Onset Strength fixed size')\n",
    "    \n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 3)\n",
    "    onset_strength_normalized = (onset_strength[20:] - np.min(onset_strength[20:])) / (np.max(onset_strength[20:]) - np.min(onset_strength[20:]))\n",
    "    plt.plot(onset_strength_normalized)\n",
    "    plt.title('Onset Strength normalized and cropped')\n",
    "    \n",
    "    # Add a plot for averaged onset strength\n",
    "    plt.subplot(rows, 1, 4)\n",
    "    averaged_onset = moving_average(onset_strength_normalized, window_size=5)  # using a window size of 10, adjust as needed\n",
    "    plt.plot(averaged_onset)\n",
    "    plt.title('Averaged Onset Strength')\n",
    "    \n",
    "    # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 5)\n",
    "    librosa.display.specshow(tempogram_fixed, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram fixed')\n",
    "    \n",
    "        # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 6)\n",
    "    librosa.display.specshow(tempogram_cropped, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram cropped')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output_processing_noise_avg/{time_signature.replace(\"/\", \"_\")}_BPM{bpm}_noise.png')\n",
    "    \n",
    "    return encoded_features[20:]\n",
    "\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for _, _, files in os.walk(directory)])\n",
    "\n",
    "# Current directory\n",
    "directory = '.'\n",
    "\n",
    "# Loop through all files in the current directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the filename ends with '.png' and contains 'spike_train'\n",
    "    if filename.endswith('.png') and 'spike_train' in filename:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Remove the file\n",
    "        os.remove(filepath)\n",
    "        print(f\"Deleted: {filename}\", end='\\r')\n",
    "        \n",
    "\n",
    "# checking shapes\n",
    "print(\"Checking shapes...\")\n",
    "fixed_timesteps = determine_fixed_length('training_data_dirty_bpm')\n",
    "print(fixed_timesteps)\n",
    "fixed_timesteps2 = determine_fixed_length('validation_data_dirty_bpm')\n",
    "print(fixed_timesteps2)\n",
    "fixed_timesteps = max(fixed_timesteps, fixed_timesteps2)\n",
    "\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "print(\"Loading and preprocessing training data...\")\n",
    "directories = ['training_data_dirty_bpm', 'validation_data_dirty_bpm']\n",
    "training_data_results, validation_data_results = parallel_data_loader(directories)\n",
    "\n",
    "training_data, training_labels = training_data_results\n",
    "validation_data, validation_labels = validation_data_results\n",
    "print(\"\\nDone with preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4: 100\n",
      "2/4: 100\n",
      "3/4: 100\n",
      "4/4: 100\n",
      "5/4: 100\n",
      "7/8: 100\n"
     ]
    }
   ],
   "source": [
    "class LIFNeuron:\n",
    "    def __init__(self, \n",
    "                 tau_m=6.0,  # Membrane Time Constant: Determines the rate at which the membrane potential decays towards its resting value.\n",
    "                 v_rest=0.0,  # Resting Potential: The stable value of the membrane potential when no external input is present.\n",
    "                 v_threshold=0.7,  # Firing Threshold: The value of the membrane potential at which the neuron generates a spike.\n",
    "                 v_reset=0.2,  # Reset Potential: The value to which the membrane potential is reset after a spike is generated.\n",
    "                 r_m=0.9,  # Membrane Resistance: The effective resistance of the neuron's membrane, modulating the influence of incoming spikes.\n",
    "                 dt=10.0,  # Time Step: Determines the granularity of the simulation time, influencing the speed of all dynamical variables.\n",
    "                 adaptive_increase=0.0,  # Adaptive Increase: The value by which the firing threshold increases after each spike.\n",
    "                 refractory_period=1  # Refractory Period: The number of time steps for which the neuron cannot fire after generating a spike.\n",
    "                ):        \n",
    "        self.tau_m = tau_m\n",
    "        self.v_rest = v_rest\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.r_m = r_m\n",
    "        self.dt = dt\n",
    "        self.v = v_rest\n",
    "        self.adaptive_increase = adaptive_increase\n",
    "        self.refractory_period = refractory_period\n",
    "        self.refractory_counter = 0\n",
    "\n",
    "    def constrain_value(self, value):\n",
    "        return min(max(value, 0.0), 1.0)\n",
    "    \n",
    "    def update_voltage(self, i):\n",
    "        dv = (-self.v + self.v_rest + self.r_m * i) / self.tau_m * self.dt\n",
    "        self.v += dv\n",
    "        self.v = self.constrain_value(self.v)\n",
    "        \n",
    "    def check_for_spike(self):\n",
    "        spike = 0\n",
    "        if self.v >= self.v_threshold:\n",
    "            spike = 1\n",
    "            self.v = self.v_reset\n",
    "            \n",
    "            if self.refractory_period > 0:\n",
    "                self.refractory_counter = self.refractory_period\n",
    "                \n",
    "            if self.adaptive_increase > 0.0:\n",
    "                self.v_threshold += self.adaptive_increase\n",
    "                self.v_threshold = self.constrain_value(self.v_threshold)\n",
    "        else:\n",
    "            if self.adaptive_increase > 0.0:\n",
    "                self.v_threshold = max(self.v_threshold - self.adaptive_increase, 1.0)\n",
    "                self.v_threshold = self.constrain_value(self.v_threshold)\n",
    "                \n",
    "        return spike\n",
    "    \n",
    "    def step(self, i):\n",
    "        if self.refractory_counter > 0:\n",
    "            self.refractory_counter -= 1\n",
    "            return 0\n",
    "\n",
    "        self.update_voltage(i)\n",
    "        \n",
    "        # Constrain all relevant variables\n",
    "        self.v_threshold = self.constrain_value(self.v_threshold)\n",
    "        self.v_reset = self.constrain_value(self.v_reset)\n",
    "        self.r_m = self.constrain_value(self.r_m)\n",
    "        self.v_rest = self.constrain_value(self.v_rest)\n",
    "        self.adaptive_increase = self.constrain_value(self.adaptive_increase)\n",
    "        \n",
    "        return self.check_for_spike()\n",
    "        \n",
    "    \n",
    "def generate_lif_spikes(data, neuron):\n",
    "    spikes = []\n",
    "    potentials = []  # To store membrane potentials\n",
    "    for i in data:\n",
    "        spike = neuron.step(i)\n",
    "        spikes.append(spike)\n",
    "        potentials.append(neuron.v)  # Store the membrane potential after each step\n",
    "    return np.array(spikes), np.array(potentials)\n",
    "\n",
    "# Pre-process training data to filter into four different lists based on time_signature\n",
    "training_data_1_4 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '1/4']\n",
    "training_data_2_4 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '2/4']\n",
    "training_data_3_4 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '3/4']\n",
    "training_data_4_4 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '4/4']\n",
    "training_data_5_4 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '5/4']\n",
    "training_data_7_8 = [item for item in training_data if item[2].split('/')[-2].replace('_', '/') == '7/8']\n",
    "\n",
    "print(f\"1/4: {len(training_data_1_4)}\")\n",
    "print(f\"2/4: {len(training_data_2_4)}\")\n",
    "print(f\"3/4: {len(training_data_3_4)}\")\n",
    "print(f\"4/4: {len(training_data_4_4)}\")\n",
    "print(f\"5/4: {len(training_data_5_4)}\")\n",
    "print(f\"7/8: {len(training_data_7_8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8086b3246b094234b4a6618e1e200b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Item No.', max=599), IntSlider(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.signal\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def smooth_using_savgol(data, window_size, polynomial_order=3):\n",
    "    return savgol_filter(data, window_size, polynomial_order)\n",
    "\n",
    "# Convert real-valued features to Poisson spike trains\n",
    "def poisson_spike_encoding(data, duration=10, dt=1*ms):\n",
    "    # Assuming data is normalized between 0 and 1\n",
    "    rates = data * (1.0/dt)\n",
    "    spikes = (np.random.rand(*data.shape) < rates*dt).astype(float)\n",
    "    return spikes\n",
    "\n",
    "def low_pass_filter(y, sr, cutoff_freq):\n",
    "    nyq = 0.5 * sr  # Nyquist frequency\n",
    "    normal_cutoff = cutoff_freq / nyq\n",
    "    b, a = scipy.signal.butter(6, normal_cutoff, btype='low', analog=False)\n",
    "    return scipy.signal.filtfilt(b, a, y)\n",
    "\n",
    "def high_pass_filter(y, sr, cutoff_freq):\n",
    "    nyq = 0.5 * sr  # Nyquist frequency\n",
    "    normal_cutoff = cutoff_freq / nyq\n",
    "    b, a = scipy.signal.butter(6, normal_cutoff, btype='high', analog=False)\n",
    "    return scipy.signal.filtfilt(b, a, y)\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalisiert eine Liste von Werten zwischen 0 und 1.\"\"\"\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    return [(val - min_val) / (max_val - min_val) for val in data]\n",
    "\n",
    "def plot_pixel_spectra_norm(item_no, window_size, tau_m, v_rest, v_threshold, v_reset, r_m, dt, high_pass_cutoff):\n",
    "    y = training_data[item_no][0]\n",
    "    sr = training_data[item_no][1]\n",
    "    file_path = training_data[item_no][2]\n",
    "    \n",
    "                # Apply a short fade-in\n",
    "    # fade_in_time = 0.000  # in seconds\n",
    "    # fade_in_samples = int(fade_in_time * sr)\n",
    "    # fade_curve = np.linspace(0, 1, fade_in_samples)\n",
    "    # y[:fade_in_samples] = y[:fade_in_samples] * fade_curve\n",
    "    # Extract the MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    # y = low_pass_filter(y, sr, low_pass_cutoff)\n",
    "    # y = high_pass_filter(y, sr, high_pass_cutoff)\n",
    "    \n",
    "    time_signature = file_path.split('/')[-2].replace('_', '/')\n",
    "    instrument, bpm, duration, noise = extract_bpm_and_instrument(file_path)\n",
    "    print(f\"time signature: {time_signature} BPM:{bpm} Noise:{noise} sampling rate: {sr} \", end='\\r')\n",
    "\n",
    "    # Extracting onset strength\n",
    "    onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_strength, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram_cropped = librosa.feature.tempogram(onset_envelope=onset_strength, sr=sr)\n",
    "    \n",
    "    # Adjust the time axis of each feature to fixed_timesteps\n",
    "    onset_strength_fixed = adjust_fixed_length(onset_strength, fixed_timesteps)\n",
    "    tempogram_fixed = adjust_fixed_length(tempogram, fixed_timesteps)\n",
    "\n",
    "    # Stacking features horizontally\n",
    "    combined_features = np.vstack(poisson_spike_encoding(onset_strength))\n",
    "    \n",
    "    # Normalize to range [0, 1]\n",
    "    encoded_features = (combined_features - np.min(combined_features)) / (np.max(combined_features) - np.min(combined_features))\n",
    "    \n",
    "    onset_strength_normalized = (onset_strength - np.min(onset_strength)) / (np.max(onset_strength) - np.min(onset_strength))\n",
    "\n",
    "    averaged_onset = moving_average(onset_strength_normalized, window_size=window_size)  # using a window size of 10, adjust as needed\n",
    "    normalized_averaged_onset = normalize_data(averaged_onset)\n",
    "    \n",
    "    global_tempo = librosa.feature.rhythm.tempo(onset_envelope=onset_strength, sr=sr)[0]\n",
    "    dtempo = librosa.feature.rhythm.tempo(onset_envelope=onset_strength, sr=sr, aggregate=None)\n",
    "    \n",
    "        # Plotting\n",
    "    plt.figure(figsize=(12, 14))\n",
    "    plt.title('audio  with {time_signature} time signature, {bpm} bpm, and instrument {instrument}')\n",
    "\n",
    "    rows = 6\n",
    "    # 1. Raw audio\n",
    "    plt.subplot(rows, 1, 1)\n",
    "    # Prepare time axes for raw audio and onset strength\n",
    "    time_audio = np.linspace(0, len(y) / sr, len(y))\n",
    "    time_onset = np.linspace(0, len(y) / sr, len(normalize_data(onset_strength)))\n",
    "\n",
    "    # Plot raw audio\n",
    "    plt.plot(time_audio, normalize_data(y), label='Raw Audio', alpha=0.7)\n",
    "\n",
    "    # Plot onset strength (scaled)\n",
    "    plt.plot(time_onset, normalize_data(onset_strength), label='Onset Strength (scaled)', alpha=0.7, color='r')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (s)')\n",
    "\n",
    "    plt.title(f'Raw Audio and Onset Strength, predicted BPM = {global_tempo}, actual BPM = {bpm}, instrument {instrument} and time signature {time_signature}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Frequency spectrum\n",
    "    plt.subplot(rows, 1, 2)\n",
    "    librosa.display.specshow(mfccs, x_axis='time')\n",
    "    plt.title('MFCC')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "    \n",
    "    \n",
    "    # 3. Frequency spectrum after high-pass filtering\n",
    "    plt.subplot(rows, 1, 3)\n",
    "    y_high_pass = high_pass_filter(y, sr, high_pass_cutoff)\n",
    "    fourier_high_pass = np.fft.fft(y_high_pass)\n",
    "    n_high_pass = len(fourier_high_pass)\n",
    "    frequencies_high_pass = np.fft.fftfreq(n_high_pass, 1/sr) \n",
    "    plt.plot(frequencies_high_pass[:n_high_pass//2], np.abs(fourier_high_pass)[:n_high_pass//2])\n",
    "    plt.title(f'Frequency Spectrum after high-pass filter at {int(high_pass_cutoff)} Hz')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "\n",
    "    \n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 4)\n",
    "    plt.plot(onset_strength_normalized, label='Onset Strength', alpha=0.7)\n",
    "    poisson_encoded = poisson_spike_encoding(onset_strength_normalized.reshape(1,-1), dt=dt*ms)\n",
    "    # Calculate the spike count\n",
    "    spike_count = np.sum(poisson_encoded)\n",
    "    plt.plot(poisson_encoded[0], label='Poisson Spike Train', linestyle=':', color='g')\n",
    "    plt.title(f'Onset Strength fixed size with Poisson Spike Train - Spike Count: {int(spike_count)}')    \n",
    "    plt.legend()\n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 5)\n",
    "    # plt.plot(onset_strength_normalized)\n",
    "   \n",
    "    lif_neuron = LIFNeuron(tau_m=tau_m, v_rest=v_rest, v_threshold=v_threshold, v_reset=v_reset, r_m=r_m, dt=dt)\n",
    "    lif_spikes = generate_lif_spikes(onset_strength_normalized, lif_neuron)\n",
    "    spike_count = np.sum(lif_spikes)\n",
    "        # 3. Onset strength with LIF spike train\n",
    "    plt.plot(onset_strength_normalized, label='Onset Strength', alpha=0.7)\n",
    "    plt.plot(lif_spikes, label='LIF Spike Train', linestyle=':', color='r')\n",
    "    plt.title(f'Onset Strength with LIF Spike Train, Spike Count: {int(spike_count)}')\n",
    "    #plt.legend()\n",
    "    \n",
    "\n",
    "    # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 6)\n",
    "    librosa.display.specshow(tempogram_fixed, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram fixed')\n",
    "\n",
    "    \n",
    "\n",
    "    plt.figtext(0.15, 0.02, f\"Item No: {item_no}, Window Size: {window_size}, tau_m: {tau_m}, v_rest: {v_rest}, \"\n",
    "                        f\"v_threshold: {v_threshold}, v_reset: {v_reset}, r_m: {r_m}, dt: {dt}, \"\n",
    "                        f\"high_pass_cutoff: {high_pass_cutoff}\", ha=\"left\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot_with_params.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def interactive_plot_spec_norm(item_no, window_size, tau_m, v_rest, v_threshold, v_reset, r_m, dt, high_pass_cutoff,):\n",
    "    plot_pixel_spectra_norm(item_no, window_size, tau_m, v_rest, v_threshold, v_reset, r_m, dt, high_pass_cutoff)\n",
    "\n",
    "\n",
    "if widgets is not None:\n",
    "    widgets.interact(\n",
    "        interactive_plot_spec_norm,\n",
    "        item_no=widgets.IntSlider(min=0, max=len(training_data)-1, value=0, step=1, continuous_update=False, description=\"Item No.\"),\n",
    "        window_size=widgets.IntSlider(min=1, max=400, value=10, step=1, continuous_update=False, description=\"avg window size\"),\n",
    "        tau_m=widgets.FloatSlider(min=1.0, max=100.0, value=5.0, step=0.5, continuous_update=False, description=\"tau_m\"),\n",
    "        v_rest=widgets.FloatSlider(min=0.0, max=1.0, value=0.0, step=0.1, continuous_update=False, description=\"v_rest\"),\n",
    "        v_threshold=widgets.FloatSlider(min=0.0, max=1.0, value=0.6, step=0.1, continuous_update=False, description=\"v_threshold\"),\n",
    "        v_reset=widgets.FloatSlider(min=0.0, max=1.0, value=0.0, step=0.1, continuous_update=False, description=\"v_reset\"),\n",
    "        r_m=widgets.FloatSlider(min=0.1, max=1.0, value=1.0, step=0.1, continuous_update=False, description=\"r_m\"),\n",
    "        dt=widgets.FloatSlider(min=0.1, max=100.0, value=10.0, step=1.0, continuous_update=False, description=\"dt\"),\n",
    "        high_pass_cutoff=widgets.FloatSlider(min=10.0, max=1000.0, value=500.0, step=10.0, continuous_update=False, description=\"high_pass_cutoff [Hz]\"),\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8139c68f73cc4aebab6e937178f94180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Item Index', max=99), FloatSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize some global variables to remember state\n",
    "last_item_index = -1\n",
    "last_onset_strengths = {'1/4': None, '2/4': None, '3/4': None, '4/4': None, '5/4': None, '7/8': None}\n",
    "\n",
    "def interactive_LIF(item_index, tau_m=6.0, v_rest=0.0, v_threshold=0.7, v_reset=0.2, r_m=0.9, dt=10.0, adaptive_increase=0.0, refractory_period=1):\n",
    "    global last_item_index, last_onset_strengths\n",
    "    \n",
    "    print(f\"item index={item_index}, last_item_index={last_item_index}\", end='\\r')\n",
    "        \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Create a title string that includes LIF parameters\n",
    "    title_str = (f\"LIF Parameters:\\n\"\n",
    "                 f\"tau_m={tau_m}, v_rest={v_rest}, v_threshold={v_threshold}, v_reset={v_reset},\\n\"\n",
    "                 f\"r_m={r_m}, dt={dt}, adaptive_increase={adaptive_increase}, refractory_period={refractory_period}\")\n",
    "    fig.suptitle(title_str, fontsize=12)\n",
    "    \n",
    "    \n",
    "    for ax, (time_sig, data) in zip(axs.flatten(), [('1/4', training_data_1_4), ('2/4', training_data_2_4), ('3/4', training_data_3_4), ('4/4', training_data_4_4), ('5/4', training_data_5_4), ('7/8', training_data_7_8)]):\n",
    "        \n",
    "        if  last_item_index != item_index or last_onset_strengths[time_sig] is None:\n",
    "            y, sr, file_path = data[item_index]\n",
    "            # Apply a short fade-in\n",
    "            # fade_in_time = 0.3  # in seconds\n",
    "            # fade_in_samples = int(fade_in_time * sr)\n",
    "            # fade_curve = np.linspace(0, 1, fade_in_samples)\n",
    "            # y[:fade_in_samples] = y[:fade_in_samples] * fade_curve\n",
    "            \n",
    "            print(\"calculating onset strength\", end='\\r')\n",
    "            onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            last_onset_strengths[time_sig] = onset_strength\n",
    "        else:\n",
    "            onset_strength = last_onset_strengths[time_sig]\n",
    "        \n",
    "        onset_strength = onset_strength[20:]\n",
    "        onset_strength_normalized = (onset_strength - np.min(onset_strength)) / (np.max(onset_strength) - np.min(onset_strength))\n",
    "\n",
    "        # Your plotting logic here using ax for plotting\n",
    "        lif_neuron = LIFNeuron(tau_m=tau_m, v_rest=v_rest, v_threshold=v_threshold, v_reset=v_reset, r_m=r_m, dt=dt, adaptive_increase=adaptive_increase, refractory_period=refractory_period)\n",
    "        lif_spikes, lif_potentials = generate_lif_spikes(onset_strength_normalized, lif_neuron)  # Note the second returned value\n",
    "        spike_count = np.sum(lif_spikes)\n",
    "        # Find indices where spikes occur\n",
    "        spike_indices = np.where(lif_spikes > 0)[0]\n",
    "        ax.vlines(spike_indices, ymin=1.05, ymax=1.1, color='r', label='LIF Spike Train')\n",
    "        ax.plot(onset_strength_normalized, label='Onset Strength', alpha=0.7)\n",
    "        ax.plot(lif_potentials, label='Membrane Potential', linestyle='--', color='g')  # Plot the potentials\n",
    "        ax.set_title(f'Onset Strength {time_sig} with LIF Spike Train, Spike Count: {int(spike_count)}')\n",
    "        ax.legend(loc='center right')\n",
    "    \n",
    "    if item_index != last_item_index:\n",
    "        last_item_index = item_index\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for the suptitle    \n",
    "    # plt.show()\n",
    "    # plot the parameters of the lif neuron\n",
    "    plt.savefig('LIF_spiketrain_compare_adaptive.png')\n",
    "\n",
    "\n",
    "max_index = min(len(training_data_1_4), len(training_data_2_4), len(training_data_3_4), len(training_data_4_4)) - 1\n",
    "\n",
    "if widgets is not None:\n",
    "    widgets.interact(\n",
    "            interactive_LIF,\n",
    "            item_index=widgets.IntSlider(min=0, max=max_index, value=0, step=1, continuous_update=False, description=\"Item Index\"),\n",
    "            tau_m=widgets.FloatSlider(min=1.0, max=10.0, value=6.0, step=0.1, continuous_update=True, description=\"tau_m\"),\n",
    "            v_rest=widgets.FloatSlider(min=0.0, max=1.0, value=0.0, step=0.1, continuous_update=True, description=\"v_rest\"),\n",
    "            v_threshold=widgets.FloatSlider(min=0.0, max=1.0, value=0.7, step=0.1, continuous_update=True, description=\"v_threshold\"),\n",
    "            v_reset=widgets.FloatSlider(min=0.0, max=1.0, value=0.2, step=0.1, continuous_update=True, description=\"v_reset\"),\n",
    "            r_m=widgets.FloatSlider(min=0.1, max=1.0, value=0.9, step=0.1, continuous_update=True, description=\"r_m\"),\n",
    "            dt=widgets.FloatSlider(min=0.1, max=100.0, value=10.0, step=0.1, continuous_update=True, description=\"dt\"),\n",
    "            adaptive_increase=widgets.FloatSlider(min=0.0, max=1.0, value=0.0, step=0.1, continuous_update=True, description=\"adaptive threshold increase\"),\n",
    "            refractory_period=widgets.IntSlider(min=0, max=10, value=1, step=1, continuous_update=True, description=\"refractory period\"),\n",
    "        );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
