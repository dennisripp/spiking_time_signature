{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking shapes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 81973.37it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 105296.33it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 100663.30it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 118149.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 30690.03it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 103138.62it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 69327.34it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 56048.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n",
      "Loading and preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 4/4 [00:00<00:00, 21.89it/s]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 4/4 [00:00<00:00, 19.18it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from brian2 import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except ImportError:\n",
    "    !pip3 install ipywidgets\n",
    "    import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fixed_timesteps = 1001\n",
    "\n",
    "def get_length(file_path):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return mfccs.shape[1]\n",
    "\n",
    "def determine_fixed_length(directory):\n",
    "    file_paths = []\n",
    "\n",
    "    for subdir in ['1_4', '2_4', '3_4', '4_4']:\n",
    "        for file in tqdm(os.listdir(os.path.join(directory, subdir))):\n",
    "            file_path = os.path.join(directory, subdir, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    # Utilize multiprocessing for faster computation\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        lengths = list(executor.map(get_length, file_paths))\n",
    "\n",
    "    return min(lengths)\n",
    "\n",
    "def parallel_data_loader(directories):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(parallel_load_and_preprocess, directories), total=len(directories)))\n",
    "    return results\n",
    "\n",
    "def load_and_preprocess_data_subdir(args):\n",
    "    directory, subdir = args\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in os.listdir(os.path.join(directory, subdir)):\n",
    "        file_path = os.path.join(directory, subdir, file)\n",
    "        processed_data = load_audio(file_path)\n",
    "        data.append(processed_data)\n",
    "        label = ['1_4', '2_4', '3_4', '4_4'].index(subdir)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def parallel_load_and_preprocess(directory):\n",
    "    # Create a pool of processes\n",
    "    pool = Pool(cpu_count())\n",
    "\n",
    "    # Create a list of tasks\n",
    "    tasks = [(directory, time_sig) for time_sig in ['1_4', '2_4', '3_4', '4_4']]\n",
    "\n",
    "    # Use imap_unordered to distribute the work among the processes\n",
    "    results = list(tqdm(pool.imap_unordered(load_and_preprocess_data_subdir, tasks), total=len(tasks), mininterval=0.01))\n",
    "\n",
    "    # Close the pool and wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results\n",
    "    combined_data = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    for data, labels in results:\n",
    "        combined_data.extend(data)\n",
    "        combined_labels.extend(labels)\n",
    "    \n",
    "    return combined_data, combined_labels\n",
    "\n",
    "\n",
    "def adjust_fixed_length(features, timesteps):\n",
    "    # If the array is 1-dimensional\n",
    "    if len(features.shape) == 1:\n",
    "        if features.shape[0] > timesteps:\n",
    "            return features[:timesteps]\n",
    "        elif features.shape[0] < timesteps:\n",
    "            padding = np.zeros(timesteps - features.shape[0])\n",
    "            return np.hstack((features, padding))\n",
    "        return features\n",
    "    # If the array is 2-dimensional\n",
    "    else:\n",
    "        # If the time axis of the 2D array is greater than timesteps, crop it.\n",
    "        if features.shape[1] > timesteps:\n",
    "            return features[:, :timesteps]\n",
    "        # If the time axis of the 2D array is less than timesteps, pad it.\n",
    "        elif features.shape[1] < timesteps:\n",
    "            padding = np.zeros((features.shape[0], timesteps - features.shape[1]))\n",
    "            return np.hstack((features, padding))\n",
    "        return features\n",
    "\n",
    "# Convert real-valued features to Poisson spike trains\n",
    "def poisson_spike_encoding(data, duration=10, dt=1*ms):\n",
    "    # Assuming data is normalized between 0 and 1\n",
    "    rates = data * (1.0/dt)\n",
    "    spikes = (np.random.rand(*data.shape) < rates*dt).astype(float)\n",
    "    return spikes\n",
    "\n",
    "def temporal_binning(data, bin_size):\n",
    "    \"\"\"\n",
    "    Bins the data into chunks of bin_size and returns the average of each chunk.\n",
    "    \"\"\"\n",
    "    # Split the data into chunks of bin_size\n",
    "    binned_data = [np.mean(data[i:i+bin_size]) for i in range(0, len(data), bin_size)]\n",
    "    return np.array(binned_data)\n",
    "\n",
    "def rate_based_encoding(data, min_freq, max_freq):\n",
    "    \"\"\"\n",
    "    Convert onset strengths to spike frequencies.\n",
    "    data: The input data (should be normalized to [0, 1])\n",
    "    min_freq: The minimum spike frequency (corresponds to data value of 0)\n",
    "    max_freq: The maximum spike frequency (corresponds to data value of 1)\n",
    "    Returns: Spike frequencies corresponding to input data\n",
    "    \"\"\"\n",
    "    return min_freq + data * (max_freq - min_freq)\n",
    "\n",
    "def extract_bpm_and_instrument(file_path):\n",
    "    match = re.search(r\"instrument_(\\d+)_bpm_(\\d+)_duration_(\\d+)_noise_([\\d.]+)\", file_path)\n",
    "    if match:\n",
    "        instrument = match.group(1)\n",
    "        bpm = match.group(2)\n",
    "        duration = match.group(3)\n",
    "        noise = match.group(4)\n",
    "        return instrument, bpm, duration, noise\n",
    "    return None, None, None, None\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute moving average\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=22050)  # setting sr ensures all files are resampled to this rate\n",
    "    return [y, sr, file_path]\n",
    "\n",
    "# Process the audio file into desired features\n",
    "# Process the audio file into desired features\n",
    "def preprocess_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=22050)  # setting sr ensures all files are resampled to this rate\n",
    "    time_signature = file_path.split('/')[-2].replace('_', '/')\n",
    "    instrument, bpm = extract_bpm_and_instrument(file_path)\n",
    "\n",
    "    # Extracting onset strength\n",
    "    onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_strength, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram_cropped = librosa.feature.tempogram(onset_envelope=onset_strength[20:], sr=sr)\n",
    "    \n",
    "    # Adjust the time axis of each feature to fixed_timesteps\n",
    "    onset_strength_fixed = adjust_fixed_length(onset_strength, fixed_timesteps)\n",
    "    tempogram_fixed = adjust_fixed_length(tempogram, fixed_timesteps)\n",
    "\n",
    "    # Stacking features horizontally\n",
    "    combined_features = np.vstack(poisson_spike_encoding(onset_strength))\n",
    "    \n",
    "    # Normalize to range [0, 1]\n",
    "    encoded_features = (combined_features - np.min(combined_features)) / (np.max(combined_features) - np.min(combined_features))\n",
    "    \n",
    "        # Plotting\n",
    "    plt.figure(figsize=(12, 14))\n",
    "    plt.title('audio  with {time_signature} time signature, {bpm} bpm, and instrument {instrument}')\n",
    "\n",
    "    rows = 6\n",
    "    # 1. Raw audio\n",
    "    plt.subplot(rows, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Raw Audio')\n",
    "\n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 2)\n",
    "    plt.plot(onset_strength_fixed)\n",
    "    plt.title('Onset Strength fixed size')\n",
    "    \n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 3)\n",
    "    onset_strength_normalized = (onset_strength[20:] - np.min(onset_strength[20:])) / (np.max(onset_strength[20:]) - np.min(onset_strength[20:]))\n",
    "    plt.plot(onset_strength_normalized)\n",
    "    plt.title('Onset Strength normalized and cropped')\n",
    "    \n",
    "    # Add a plot for averaged onset strength\n",
    "    plt.subplot(rows, 1, 4)\n",
    "    averaged_onset = moving_average(onset_strength_normalized, window_size=5)  # using a window size of 10, adjust as needed\n",
    "    plt.plot(averaged_onset)\n",
    "    plt.title('Averaged Onset Strength')\n",
    "    \n",
    "    # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 5)\n",
    "    librosa.display.specshow(tempogram_fixed, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram fixed')\n",
    "    \n",
    "        # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 6)\n",
    "    librosa.display.specshow(tempogram_cropped, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram cropped')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output_processing_noise_avg/{time_signature.replace(\"/\", \"_\")}_BPM{bpm}_noise.png')\n",
    "    \n",
    "    return encoded_features[20:]\n",
    "\n",
    "\n",
    "def count_files(directory):\n",
    "    return sum([len(files) for _, _, files in os.walk(directory)])\n",
    "\n",
    "# Current directory\n",
    "directory = '.'\n",
    "\n",
    "# Loop through all files in the current directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the filename ends with '.png' and contains 'spike_train'\n",
    "    if filename.endswith('.png') and 'spike_train' in filename:\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Remove the file\n",
    "        os.remove(filepath)\n",
    "        print(f\"Deleted: {filename}\", end='\\r')\n",
    "        \n",
    "\n",
    "# checking shapes\n",
    "print(\"Checking shapes...\")\n",
    "fixed_timesteps = determine_fixed_length('training_data')\n",
    "print(fixed_timesteps)\n",
    "fixed_timesteps2 = determine_fixed_length('validation_data')\n",
    "print(fixed_timesteps2)\n",
    "fixed_timesteps = max(fixed_timesteps, fixed_timesteps2)\n",
    "\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "print(\"Loading and preprocessing training data...\")\n",
    "directories = ['training_data', 'validation_data']\n",
    "training_data_results, validation_data_results = parallel_data_loader(directories)\n",
    "\n",
    "training_data, training_labels = training_data_results\n",
    "validation_data, validation_labels = validation_data_results\n",
    "print(\"\\nDone with preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda82e5b99544c6091926e4f9d5fae6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Item No.', max=23), IntSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.signal\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def smooth_using_savgol(data, window_size, polynomial_order=3):\n",
    "    return savgol_filter(data, window_size, polynomial_order)\n",
    "\n",
    "\n",
    "def low_pass_filter(y, sr, cutoff_freq):\n",
    "    nyq = 0.5 * sr  # Nyquist frequency\n",
    "    normal_cutoff = cutoff_freq / nyq\n",
    "    b, a = scipy.signal.butter(6, normal_cutoff, btype='low', analog=False)\n",
    "    return scipy.signal.filtfilt(b, a, y)\n",
    "\n",
    "def high_pass_filter(y, sr, cutoff_freq):\n",
    "    nyq = 0.5 * sr  # Nyquist frequency\n",
    "    normal_cutoff = cutoff_freq / nyq\n",
    "    b, a = scipy.signal.butter(6, normal_cutoff, btype='high', analog=False)\n",
    "    return scipy.signal.filtfilt(b, a, y)\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalisiert eine Liste von Werten zwischen 0 und 1.\"\"\"\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    return [(val - min_val) / (max_val - min_val) for val in data]\n",
    "\n",
    "def plot_pixel_spectra_norm(item_no, window_size, low_pass_cutoff=500, high_pass_cutoff=500, poly_order=3, avg_golay=10):\n",
    "    y = training_data[item_no][0]\n",
    "    sr = training_data[item_no][1]\n",
    "    file_path = training_data[item_no][2]\n",
    "    y = low_pass_filter(y, sr, low_pass_cutoff)\n",
    "    y = high_pass_filter(y, sr, high_pass_cutoff)\n",
    "    \n",
    "    time_signature = file_path.split('/')[-2].replace('_', '/')\n",
    "    instrument, bpm, duration, noise = extract_bpm_and_instrument(file_path)\n",
    "    print(f\"time signature: {time_signature} BPM:{bpm} Noise:{noise} \", end='\\r')\n",
    "\n",
    "    # Extracting onset strength\n",
    "    onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_strength, sr=sr)\n",
    "    \n",
    "    # Extracting tempogram\n",
    "    tempogram_cropped = librosa.feature.tempogram(onset_envelope=onset_strength[20:], sr=sr)\n",
    "    \n",
    "    # Adjust the time axis of each feature to fixed_timesteps\n",
    "    onset_strength_fixed = adjust_fixed_length(onset_strength, fixed_timesteps)\n",
    "    tempogram_fixed = adjust_fixed_length(tempogram, fixed_timesteps)\n",
    "\n",
    "    # Stacking features horizontally\n",
    "    combined_features = np.vstack(poisson_spike_encoding(onset_strength))\n",
    "    \n",
    "    # Normalize to range [0, 1]\n",
    "    encoded_features = (combined_features - np.min(combined_features)) / (np.max(combined_features) - np.min(combined_features))\n",
    "    \n",
    "    onset_strength_normalized = (onset_strength[20:] - np.min(onset_strength[20:])) / (np.max(onset_strength[20:]) - np.min(onset_strength[20:]))\n",
    "\n",
    "    averaged_onset = moving_average(onset_strength_normalized, window_size=window_size)  # using a window size of 10, adjust as needed\n",
    "    normalized_averaged_onset = normalize_data(averaged_onset)\n",
    "    \n",
    "    global_tempo = librosa.feature.rhythm.tempo(onset_envelope=onset_strength, sr=sr)[0]\n",
    "    dtempo = librosa.feature.rhythm.tempo(onset_envelope=onset_strength, sr=sr, aggregate=None)\n",
    "    \n",
    "        # Plotting\n",
    "    plt.figure(figsize=(12, 14))\n",
    "    plt.title('audio  with {time_signature} time signature, {bpm} bpm, and instrument {instrument}')\n",
    "\n",
    "    rows = 6\n",
    "    # 1. Raw audio\n",
    "    plt.subplot(rows, 1, 1)\n",
    "    librosa.display.waveshow(y[20:], sr=sr)\n",
    "    plt.title('Raw Audio,predicted BPM = ' + str(global_tempo) + ' actual BPM = ' + str(bpm))\n",
    "\n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 2)\n",
    "    plt.plot(onset_strength_fixed)\n",
    "    plt.title('Onset Strength fixed size')\n",
    "    \n",
    "    # 2. Onset strength\n",
    "    plt.subplot(rows, 1, 3)\n",
    "    # plt.plot(onset_strength_normalized)\n",
    "    plt.plot(smooth_using_savgol(onset_strength_normalized, avg_golay, polynomial_order=poly_order))\n",
    "    plt.title('Onset Strength normalized and cropped')\n",
    "    \n",
    "    # Add a plot for averaged onset strength\n",
    "    plt.subplot(rows, 1, 4)\n",
    "    plt.plot(normalized_averaged_onset)\n",
    "    plt.title('Averaged Onset Strength')\n",
    "    \n",
    "    # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 5)\n",
    "    librosa.display.specshow(tempogram_fixed, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram fixed')\n",
    "    \n",
    "        # 3. Tempogram\n",
    "    plt.subplot(rows, 1, 6)\n",
    "    librosa.display.specshow(tempogram_cropped, sr=sr, x_axis='time', y_axis='tempo')\n",
    "    plt.title('Tempogram cropped')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def interactive_plot_spec_norm(item_no, window_size, low_pass_cutoff=500, high_pass_cutoff=500, poly_order=3, avg_golay=10):\n",
    "    plot_pixel_spectra_norm(item_no, window_size, low_pass_cutoff=500, high_pass_cutoff=500, poly_order=3, avg_golay=10)\n",
    "\n",
    "if widgets is not None:\n",
    "    widgets.interact(\n",
    "        interactive_plot_spec_norm,\n",
    "        item_no=widgets.IntSlider(min=0, max=len(training_data)-1, value=0, step=1, continuous_update=False, description=\"Item No.\"),\n",
    "        window_size=widgets.IntSlider(min=1, max=400, value=10, step=1, continuous_update=False, description=\"avg window size\"),\n",
    "        low_pass_cutoff=widgets.IntSlider(min=0, max=20000, value=0, step=1, continuous_update=False, description=\"low pass cutoff\"),\n",
    "        high_pass_cutoff=widgets.IntSlider(min=0, max=120, value=0, step=1, continuous_update=False, description=\"high pass cutoff\"),\n",
    "        poly_order=widgets.IntSlider(min=1, max=10, value=3, step=1, continuous_update=False, description=\"poly order\"),\n",
    "        avg_golay=widgets.IntSlider(min=1, max=20, value=10, step=1, continuous_update=False, description=\"golay size\"),\n",
    "    );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
